{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>OpenBB Chat provides chat capabilities to OpenBB by leveraging the generative potential of LLMs. The chat is implemented following InstructGPT. This repository contains the implementations of the NLP models and the training/inference infraestructure.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#poetry","title":"Poetry","text":"<pre><code># clone project\ngit clone https://github.com/Dedalo314/openbb-chat\ncd openbb-chat\n\n# [OPTIONAL] create conda environment\nconda create -n myenv python=3.10\nconda activate myenv\n\n# install poetry (change paths as needed)\nPOETRY_VERSION=1.5.1\nPOETRY_HOME=/opt/poetry\nPOETRY_VENV=/opt/poetry-venv\nPOETRY_CACHE_DIR=/opt/.cache\npython3 -m venv $POETRY_VENV \\\n&amp;&amp; $POETRY_VENV/bin/pip install -U pip setuptools \\\n&amp;&amp; $POETRY_VENV/bin/pip install poetry==${POETRY_VERSION}\n\n# add poetry to PATH\nPATH=\"${PATH}:${POETRY_VENV}/bin\"\n\npoetry install\n</code></pre>"},{"location":"#how-to-run","title":"How to run","text":"<p>Train model with default configuration</p> <pre><code># train demo on CPU\npoetry run python openbb_chat/train.py trainer=cpu\n\n# train demo on GPU\npoetry run python openbb_chat/train.py trainer=gpu\n</code></pre> <p>Train model with chosen experiment configuration from configs/experiment/</p> <pre><code>poetry run python openbb_chat/train.py experiment=experiment_name.yaml\n</code></pre> <p>You can override any parameter from command line like this</p> <pre><code>poetry run python openbb_chat/train.py trainer.max_epochs=20 data.batch_size=64\n</code></pre>"},{"location":"#released-models","title":"Released models","text":"<p>The model Griffin-3B-GPTQ has been created as part of this project by quantizing Griffin-3B. In the future, more models will be trained and released as needed.</p>"},{"location":"#sample-usage-with-pre-trained-models","title":"Sample usage with pre-trained models","text":"<p>In the repository https://github.com/GPTStonks/api <code>openbb-chat</code> is used to perform retrieval-augmented generation (RAG) with OpenBB's official documentation and pre-trained models. In particular, the <code>classifiers</code> modules are used to find the appropriate function in OpenBB and the <code>llms</code> modules are used to complete the function call.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>openbb_chat<ul> <li>classifiers<ul> <li>abstract_zeroshot_classifier</li> <li>roberta</li> <li>stransformer</li> </ul> </li> <li>data<ul> <li>components</li> <li>mnist_datamodule</li> </ul> </li> <li>eval</li> <li>llms<ul> <li>guidance_wrapper</li> </ul> </li> <li>models<ul> <li>components<ul> <li>simple_dense_net</li> </ul> </li> <li>mnist_module</li> </ul> </li> <li>train</li> <li>utils<ul> <li>instantiators</li> <li>logging_utils</li> <li>pylogger</li> <li>rich_utils</li> <li>utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/openbb_chat/","title":"Index","text":""},{"location":"reference/openbb_chat/eval/","title":"eval","text":""},{"location":"reference/openbb_chat/eval/#openbb_chat.eval.evaluate","title":"<code>evaluate(cfg)</code>","text":"<p>Evaluates given checkpoint on a datamodule testset.</p> <p>This method is wrapped in optional @task_wrapper decorator, that controls the behavior during failure. Useful for multiruns, saving info about the crash, etc.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration composed by Hydra.</p> required <p>Returns:</p> Type Description <code>Tuple[dict, dict]</code> <p>Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.</p> Source code in <code>openbb_chat/eval.py</code> <pre><code>@utils.task_wrapper\ndef evaluate(cfg: DictConfig) -&gt; Tuple[dict, dict]:\n\"\"\"Evaluates given checkpoint on a datamodule testset.\n\n    This method is wrapped in optional @task_wrapper decorator, that controls the behavior during\n    failure. Useful for multiruns, saving info about the crash, etc.\n\n    Args:\n        cfg (DictConfig): Configuration composed by Hydra.\n\n    Returns:\n        Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.\n    \"\"\"\n\n    assert cfg.ckpt_path\n\n    log.info(f\"Instantiating datamodule &lt;{cfg.data._target_}&gt;\")\n    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)\n\n    log.info(f\"Instantiating model &lt;{cfg.model._target_}&gt;\")\n    model: LightningModule = hydra.utils.instantiate(cfg.model)\n\n    log.info(\"Instantiating loggers...\")\n    logger: List[Logger] = utils.instantiate_loggers(cfg.get(\"logger\"))\n\n    log.info(f\"Instantiating trainer &lt;{cfg.trainer._target_}&gt;\")\n    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, logger=logger)\n\n    object_dict = {\n        \"cfg\": cfg,\n        \"datamodule\": datamodule,\n        \"model\": model,\n        \"logger\": logger,\n        \"trainer\": trainer,\n    }\n\n    if logger:\n        log.info(\"Logging hyperparameters!\")\n        utils.log_hyperparameters(object_dict)\n\n    log.info(\"Starting testing!\")\n    trainer.test(model=model, datamodule=datamodule, ckpt_path=cfg.ckpt_path)\n\n    # for predictions use trainer.predict(...)\n    # predictions = trainer.predict(model=model, dataloaders=dataloaders, ckpt_path=cfg.ckpt_path)\n\n    metric_dict = trainer.callback_metrics\n\n    return metric_dict, object_dict\n</code></pre>"},{"location":"reference/openbb_chat/train/","title":"train","text":""},{"location":"reference/openbb_chat/train/#openbb_chat.train.train","title":"<code>train(cfg)</code>","text":"<p>Trains the model. Can additionally evaluate on a testset, using best weights obtained during training.</p> <p>This method is wrapped in optional @task_wrapper decorator, that controls the behavior during failure. Useful for multiruns, saving info about the crash, etc.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration composed by Hydra.</p> required <p>Returns:</p> Type Description <code>Tuple[dict, dict]</code> <p>Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.</p> Source code in <code>openbb_chat/train.py</code> <pre><code>@utils.task_wrapper\ndef train(cfg: DictConfig) -&gt; Tuple[dict, dict]:\n\"\"\"Trains the model. Can additionally evaluate on a testset, using best weights obtained during\n    training.\n\n    This method is wrapped in optional @task_wrapper decorator, that controls the behavior during\n    failure. Useful for multiruns, saving info about the crash, etc.\n\n    Args:\n        cfg (DictConfig): Configuration composed by Hydra.\n\n    Returns:\n        Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.\n    \"\"\"\n\n    # set seed for random number generators in pytorch, numpy and python.random\n    if cfg.get(\"seed\"):\n        L.seed_everything(cfg.seed, workers=True)\n\n    log.info(f\"Instantiating datamodule &lt;{cfg.data._target_}&gt;\")\n    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)\n\n    log.info(f\"Instantiating model &lt;{cfg.model._target_}&gt;\")\n    model: LightningModule = hydra.utils.instantiate(cfg.model)\n\n    log.info(\"Instantiating callbacks...\")\n    callbacks: List[Callback] = utils.instantiate_callbacks(cfg.get(\"callbacks\"))\n\n    log.info(\"Instantiating loggers...\")\n    logger: List[Logger] = utils.instantiate_loggers(cfg.get(\"logger\"))\n\n    log.info(f\"Instantiating trainer &lt;{cfg.trainer._target_}&gt;\")\n    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, callbacks=callbacks, logger=logger)\n\n    object_dict = {\n        \"cfg\": cfg,\n        \"datamodule\": datamodule,\n        \"model\": model,\n        \"callbacks\": callbacks,\n        \"logger\": logger,\n        \"trainer\": trainer,\n    }\n\n    if logger:\n        log.info(\"Logging hyperparameters!\")\n        utils.log_hyperparameters(object_dict)\n\n    if cfg.get(\"compile\"):\n        log.info(\"Compiling model!\")\n        model = torch.compile(model)\n\n    if cfg.get(\"train\"):\n        log.info(\"Starting training!\")\n        trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get(\"ckpt_path\"))\n\n    train_metrics = trainer.callback_metrics\n\n    if cfg.get(\"test\"):\n        log.info(\"Starting testing!\")\n        ckpt_path = trainer.checkpoint_callback.best_model_path\n        if ckpt_path == \"\":\n            log.warning(\"Best ckpt not found! Using current weights for testing...\")\n            ckpt_path = None\n        trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)\n        log.info(f\"Best ckpt path: {ckpt_path}\")\n\n    test_metrics = trainer.callback_metrics\n\n    # merge train and test metrics\n    metric_dict = {**train_metrics, **test_metrics}\n\n    return metric_dict, object_dict\n</code></pre>"},{"location":"reference/openbb_chat/classifiers/","title":"Index","text":""},{"location":"reference/openbb_chat/classifiers/abstract_zeroshot_classifier/","title":"abstract_zeroshot_classifier","text":""},{"location":"reference/openbb_chat/classifiers/abstract_zeroshot_classifier/#openbb_chat.classifiers.abstract_zeroshot_classifier.AbstractZeroshotClassifier","title":"<code>AbstractZeroshotClassifier</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract parent class of zero-shot classifiers.</p> <p>It instantiates the necessary transformers models and loads the keys into an embedding matrix.</p> Source code in <code>openbb_chat/classifiers/abstract_zeroshot_classifier.py</code> <pre><code>class AbstractZeroshotClassifier(ABC):\n\"\"\"Abstract parent class of zero-shot classifiers.\n\n    It instantiates the necessary transformers models and loads the keys into an embedding matrix.\n    \"\"\"\n\n    def __init__(\n        self,\n        keys: List[str],\n        model_id: str,\n        use_automodel_for_seq: bool = False,\n        model_kwargs: dict = {},\n        tokenizer_kwargs: dict = {},\n    ):\n\"\"\"Init method.\n\n        Args:\n            model_id (`str`):\n                Name of the HF model to use.\n            keys (`List[str]`):\n                All possible classes, each one a short description.\n            use_automodel_for_seq (`bool`):\n                Whether to use `AutoModelForSequenceClassification` or `AutoModel` (default).\n        \"\"\"\n        self.keys = keys\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id, **tokenizer_kwargs)\n        if not use_automodel_for_seq:\n            self.model = AutoModel.from_pretrained(model_id, **model_kwargs)\n        else:\n            self.model = AutoModelForSequenceClassification.from_pretrained(\n                model_id, **model_kwargs\n            )\n        self.model.eval()\n\n        descr_embed_list = []\n        for descr in self.keys:\n            inputs = self.tokenizer(descr, return_tensors=\"pt\")\n            descr_embed = self._compute_embed(inputs)\n            descr_embed_list.append(descr_embed)\n\n        self.descr_embed = torch.nn.functional.normalize(torch.cat(descr_embed_list), dim=1)\n\n    def classify(self, query: str) -&gt; Tuple[str, float, int]:\n\"\"\"Given a query, the most similar key is returned.\n\n        Args:\n            query (`str`):\n                Short text to classify into one key.\n\n        Returns:\n            `str`: The most similar key.\n            `float`: The score obtained.\n            `int`: Index of the key.\n        \"\"\"\n\n        inputs = self.tokenizer(query, return_tensors=\"pt\")\n        target_embed = torch.nn.functional.normalize(self._compute_embed(inputs))\n        cosine_similarities = torch.sum(target_embed * self.descr_embed, dim=1)\n        selected_index = torch.argmax(cosine_similarities)\n        most_sim_descr = self.keys[selected_index]\n        max_cosine_sim = torch.max(cosine_similarities)\n\n        return most_sim_descr, max_cosine_sim, selected_index\n\n    def rank_k(self, query: str, k: int = 3) -&gt; Tuple[List[str], List[float], List[int]]:\n\"\"\"Given a query, the k most similar keys are returned, in descending order of scores.\n\n        Args:\n            query (`str`):\n                Short text to classify into one key.\n            k (`int`):\n                No. keys to return.\n\n        Returns:\n            `List[str]`: The most similar keys.\n            `List[float]`: The scores obtained.\n            `List[int]`: Indices of the key.\n        \"\"\"\n        inputs = self.tokenizer(query, return_tensors=\"pt\")\n        target_embed = torch.nn.functional.normalize(self._compute_embed(inputs))\n        cosine_similarities = torch.sum(target_embed * self.descr_embed, dim=1)\n        selected_scores, selected_indices = torch.topk(cosine_similarities, k=k)\n        selected_scores = selected_scores.cpu().numpy().tolist()\n        selected_indices = selected_indices.cpu().numpy().tolist()\n        most_sim_descrs = [self.keys[selected_index] for selected_index in selected_indices]\n\n        return most_sim_descrs, selected_scores, selected_indices\n\n    @abstractmethod\n    def _compute_embed(self, inputs: dict) -&gt; torch.Tensor:\n\"\"\"Computes the final embedding of the text given the tokenized inputs and the model.\n\n        Args:\n            inputs (`dict`): tokenized inputs to the model.\n\n        Returns:\n            `torch.Tensor`: embedding of the text. Shape (1, model_dim).\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/openbb_chat/classifiers/abstract_zeroshot_classifier/#openbb_chat.classifiers.abstract_zeroshot_classifier.AbstractZeroshotClassifier.__init__","title":"<code>__init__(keys, model_id, use_automodel_for_seq=False, model_kwargs={}, tokenizer_kwargs={})</code>","text":"<p>Init method.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>`str`</code> <p>Name of the HF model to use.</p> required <code>keys</code> <code>`List[str]`</code> <p>All possible classes, each one a short description.</p> required <code>use_automodel_for_seq</code> <code>`bool`</code> <p>Whether to use <code>AutoModelForSequenceClassification</code> or <code>AutoModel</code> (default).</p> <code>False</code> Source code in <code>openbb_chat/classifiers/abstract_zeroshot_classifier.py</code> <pre><code>def __init__(\n    self,\n    keys: List[str],\n    model_id: str,\n    use_automodel_for_seq: bool = False,\n    model_kwargs: dict = {},\n    tokenizer_kwargs: dict = {},\n):\n\"\"\"Init method.\n\n    Args:\n        model_id (`str`):\n            Name of the HF model to use.\n        keys (`List[str]`):\n            All possible classes, each one a short description.\n        use_automodel_for_seq (`bool`):\n            Whether to use `AutoModelForSequenceClassification` or `AutoModel` (default).\n    \"\"\"\n    self.keys = keys\n    self.tokenizer = AutoTokenizer.from_pretrained(model_id, **tokenizer_kwargs)\n    if not use_automodel_for_seq:\n        self.model = AutoModel.from_pretrained(model_id, **model_kwargs)\n    else:\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            model_id, **model_kwargs\n        )\n    self.model.eval()\n\n    descr_embed_list = []\n    for descr in self.keys:\n        inputs = self.tokenizer(descr, return_tensors=\"pt\")\n        descr_embed = self._compute_embed(inputs)\n        descr_embed_list.append(descr_embed)\n\n    self.descr_embed = torch.nn.functional.normalize(torch.cat(descr_embed_list), dim=1)\n</code></pre>"},{"location":"reference/openbb_chat/classifiers/abstract_zeroshot_classifier/#openbb_chat.classifiers.abstract_zeroshot_classifier.AbstractZeroshotClassifier.classify","title":"<code>classify(query)</code>","text":"<p>Given a query, the most similar key is returned.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>`str`</code> <p>Short text to classify into one key.</p> required <p>Returns:</p> Type Description <code>str</code> <p><code>str</code>: The most similar key.</p> <code>float</code> <p><code>float</code>: The score obtained.</p> <code>int</code> <p><code>int</code>: Index of the key.</p> Source code in <code>openbb_chat/classifiers/abstract_zeroshot_classifier.py</code> <pre><code>def classify(self, query: str) -&gt; Tuple[str, float, int]:\n\"\"\"Given a query, the most similar key is returned.\n\n    Args:\n        query (`str`):\n            Short text to classify into one key.\n\n    Returns:\n        `str`: The most similar key.\n        `float`: The score obtained.\n        `int`: Index of the key.\n    \"\"\"\n\n    inputs = self.tokenizer(query, return_tensors=\"pt\")\n    target_embed = torch.nn.functional.normalize(self._compute_embed(inputs))\n    cosine_similarities = torch.sum(target_embed * self.descr_embed, dim=1)\n    selected_index = torch.argmax(cosine_similarities)\n    most_sim_descr = self.keys[selected_index]\n    max_cosine_sim = torch.max(cosine_similarities)\n\n    return most_sim_descr, max_cosine_sim, selected_index\n</code></pre>"},{"location":"reference/openbb_chat/classifiers/abstract_zeroshot_classifier/#openbb_chat.classifiers.abstract_zeroshot_classifier.AbstractZeroshotClassifier.rank_k","title":"<code>rank_k(query, k=3)</code>","text":"<p>Given a query, the k most similar keys are returned, in descending order of scores.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>`str`</code> <p>Short text to classify into one key.</p> required <code>k</code> <code>`int`</code> <p>No. keys to return.</p> <code>3</code> <p>Returns:</p> Type Description <code>List[str]</code> <p><code>List[str]</code>: The most similar keys.</p> <code>List[float]</code> <p><code>List[float]</code>: The scores obtained.</p> <code>List[int]</code> <p><code>List[int]</code>: Indices of the key.</p> Source code in <code>openbb_chat/classifiers/abstract_zeroshot_classifier.py</code> <pre><code>def rank_k(self, query: str, k: int = 3) -&gt; Tuple[List[str], List[float], List[int]]:\n\"\"\"Given a query, the k most similar keys are returned, in descending order of scores.\n\n    Args:\n        query (`str`):\n            Short text to classify into one key.\n        k (`int`):\n            No. keys to return.\n\n    Returns:\n        `List[str]`: The most similar keys.\n        `List[float]`: The scores obtained.\n        `List[int]`: Indices of the key.\n    \"\"\"\n    inputs = self.tokenizer(query, return_tensors=\"pt\")\n    target_embed = torch.nn.functional.normalize(self._compute_embed(inputs))\n    cosine_similarities = torch.sum(target_embed * self.descr_embed, dim=1)\n    selected_scores, selected_indices = torch.topk(cosine_similarities, k=k)\n    selected_scores = selected_scores.cpu().numpy().tolist()\n    selected_indices = selected_indices.cpu().numpy().tolist()\n    most_sim_descrs = [self.keys[selected_index] for selected_index in selected_indices]\n\n    return most_sim_descrs, selected_scores, selected_indices\n</code></pre>"},{"location":"reference/openbb_chat/classifiers/roberta/","title":"roberta","text":""},{"location":"reference/openbb_chat/classifiers/roberta/#openbb_chat.classifiers.roberta.RoBERTaZeroshotClassifier","title":"<code>RoBERTaZeroshotClassifier</code>","text":"<p>             Bases: <code>AbstractZeroshotClassifier</code></p> <p>Zero-shot classifier based on <code>sentence-transformers</code>.</p> Source code in <code>openbb_chat/classifiers/roberta.py</code> <pre><code>class RoBERTaZeroshotClassifier(AbstractZeroshotClassifier):\n\"\"\"Zero-shot classifier based on `sentence-transformers`.\"\"\"\n\n    def __init__(self, keys: List[str], model_id: str = \"roberta-base\", *args, **kwargs):\n\"\"\"Override __init__ to set default model_id.\"\"\"\n        super().__init__(keys, model_id, *args, **kwargs)\n\n    def _compute_embed(self, inputs: dict) -&gt; torch.Tensor:\n\"\"\"Override parent method to use RoBERTa pooler output ([CLS] token).\"\"\"\n        return self.model(**inputs).pooler_output\n</code></pre>"},{"location":"reference/openbb_chat/classifiers/roberta/#openbb_chat.classifiers.roberta.RoBERTaZeroshotClassifier.__init__","title":"<code>__init__(keys, model_id='roberta-base', *args, **kwargs)</code>","text":"<p>Override init to set default model_id.</p> Source code in <code>openbb_chat/classifiers/roberta.py</code> <pre><code>def __init__(self, keys: List[str], model_id: str = \"roberta-base\", *args, **kwargs):\n\"\"\"Override __init__ to set default model_id.\"\"\"\n    super().__init__(keys, model_id, *args, **kwargs)\n</code></pre>"},{"location":"reference/openbb_chat/classifiers/stransformer/","title":"stransformer","text":""},{"location":"reference/openbb_chat/classifiers/stransformer/#openbb_chat.classifiers.stransformer.STransformerZeroshotClassifier","title":"<code>STransformerZeroshotClassifier</code>","text":"<p>             Bases: <code>AbstractZeroshotClassifier</code></p> <p>Zero-shot classifier based on <code>sentence-transformers</code>.</p> Source code in <code>openbb_chat/classifiers/stransformer.py</code> <pre><code>class STransformerZeroshotClassifier(AbstractZeroshotClassifier):\n\"\"\"Zero-shot classifier based on `sentence-transformers`.\"\"\"\n\n    def __init__(\n        self,\n        keys: List[str],\n        model_id: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n        pooling_type: str = \"average\",\n        *args,\n        **kwargs,\n    ):\n\"\"\"Override __init__ to set default model_id.\n\n        Args:\n            pooling_type (`str`):\n                Type of pooling to apply to output embeddings of the model. Only \"average\" and \"cls\" supported.\n        \"\"\"\n        self.pooling_type = pooling_type\n        super().__init__(keys, model_id, *args, **kwargs)\n\n    def _compute_embed(self, inputs: dict) -&gt; torch.Tensor:\n\"\"\"Override parent method to use `sentence-transformers` models in HF.\"\"\"\n        if self.pooling_type == \"average\":\n            return self._mean_pooling(self.model(**inputs), inputs[\"attention_mask\"])\n        elif self.pooling_type == \"cls\":\n            return self._cls_pooling(self.model(**inputs))\n        else:\n            raise ValueError(\n                f\"pooling_type can only be 'average' or 'cls', but current value is {self.pooling_type}\"\n            )\n\n    def _mean_pooling(self, model_output: object, attention_mask: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"\n        Mean Pooling - Take attention mask into account for correct averaging.\n\n        Args:\n            model_output (`object`): output of `sentence-transformers` model in HF.\n            attention_mask (`torch.Tensor`): attention mask denoting padding.\n\n        Returns:\n            `torch.Tensor`: final embedding computed from the model output.\n        \"\"\"\n        # Code from https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n        token_embeddings = model_output[\n            0\n        ]  # First element of model_output contains all token embeddings\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n            input_mask_expanded.sum(1), min=1e-9\n        )\n\n    def _cls_pooling(self, model_output: object) -&gt; torch.Tensor:\n\"\"\"\n        CLS Pooling - Only CLS embedding used as an overall representation of the text.\n\n        Args:\n            model_output (`object`): output of `sentence-transformers` model in HF.\n\n        Returns:\n            `torch.Tensor`: final embedding computed from the model output.\n        \"\"\"\n        # Code from https://huggingface.co/BAAI/bge-base-en\n        sentence_embeddings = model_output[0][:, 0]\n        # Normalize embeddings\n        return torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n</code></pre>"},{"location":"reference/openbb_chat/classifiers/stransformer/#openbb_chat.classifiers.stransformer.STransformerZeroshotClassifier.__init__","title":"<code>__init__(keys, model_id='sentence-transformers/all-MiniLM-L6-v2', pooling_type='average', *args, **kwargs)</code>","text":"<p>Override init to set default model_id.</p> <p>Parameters:</p> Name Type Description Default <code>pooling_type</code> <code>`str`</code> <p>Type of pooling to apply to output embeddings of the model. Only \"average\" and \"cls\" supported.</p> <code>'average'</code> Source code in <code>openbb_chat/classifiers/stransformer.py</code> <pre><code>def __init__(\n    self,\n    keys: List[str],\n    model_id: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n    pooling_type: str = \"average\",\n    *args,\n    **kwargs,\n):\n\"\"\"Override __init__ to set default model_id.\n\n    Args:\n        pooling_type (`str`):\n            Type of pooling to apply to output embeddings of the model. Only \"average\" and \"cls\" supported.\n    \"\"\"\n    self.pooling_type = pooling_type\n    super().__init__(keys, model_id, *args, **kwargs)\n</code></pre>"},{"location":"reference/openbb_chat/data/","title":"Index","text":""},{"location":"reference/openbb_chat/data/mnist_datamodule/","title":"mnist_datamodule","text":""},{"location":"reference/openbb_chat/data/mnist_datamodule/#openbb_chat.data.mnist_datamodule.MNISTDataModule","title":"<code>MNISTDataModule</code>","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>Example of LightningDataModule for MNIST dataset.</p> A DataModule implements 6 key methods <p>def prepare_data(self):     # things to do on 1 GPU/TPU (not on every GPU/TPU in DDP)     # download data, pre-process, split, save to disk, etc... def setup(self, stage):     # things to do on every process in DDP     # load data, set variables, etc... def train_dataloader(self):     # return train dataloader def val_dataloader(self):     # return validation dataloader def test_dataloader(self):     # return test dataloader def teardown(self):     # called on every process in DDP     # clean up after fit or test</p> <p>This allows you to share a full dataset without explaining how to download, split, transform and process the data.</p> Read the docs <p>https://lightning.ai/docs/pytorch/latest/data/datamodule.html</p> Source code in <code>openbb_chat/data/mnist_datamodule.py</code> <pre><code>class MNISTDataModule(LightningDataModule):\n\"\"\"Example of LightningDataModule for MNIST dataset.\n\n    A DataModule implements 6 key methods:\n        def prepare_data(self):\n            # things to do on 1 GPU/TPU (not on every GPU/TPU in DDP)\n            # download data, pre-process, split, save to disk, etc...\n        def setup(self, stage):\n            # things to do on every process in DDP\n            # load data, set variables, etc...\n        def train_dataloader(self):\n            # return train dataloader\n        def val_dataloader(self):\n            # return validation dataloader\n        def test_dataloader(self):\n            # return test dataloader\n        def teardown(self):\n            # called on every process in DDP\n            # clean up after fit or test\n\n    This allows you to share a full dataset without explaining how to download,\n    split, transform and process the data.\n\n    Read the docs:\n        https://lightning.ai/docs/pytorch/latest/data/datamodule.html\n    \"\"\"\n\n    def __init__(\n        self,\n        data_dir: str = \"data/\",\n        train_val_test_split: Tuple[int, int, int] = (55_000, 5_000, 10_000),\n        batch_size: int = 64,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        # data transformations\n        self.transforms = transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n        )\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n        self.data_test: Optional[Dataset] = None\n\n    @property\n    def num_classes(self):\n        return 10\n\n    def prepare_data(self):\n\"\"\"Download data if needed.\n\n        Do not use it to assign state (self.x = y).\n        \"\"\"\n        MNIST(self.hparams.data_dir, train=True, download=True)\n        MNIST(self.hparams.data_dir, train=False, download=True)\n\n    def setup(self, stage: Optional[str] = None):\n\"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\n        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n        careful not to execute things like random split twice!\n        \"\"\"\n        # load and split datasets only if not loaded already\n        if not self.data_train and not self.data_val and not self.data_test:\n            trainset = MNIST(self.hparams.data_dir, train=True, transform=self.transforms)\n            testset = MNIST(self.hparams.data_dir, train=False, transform=self.transforms)\n            dataset = ConcatDataset(datasets=[trainset, testset])\n            self.data_train, self.data_val, self.data_test = random_split(\n                dataset=dataset,\n                lengths=self.hparams.train_val_test_split,\n                generator=torch.Generator().manual_seed(42),\n            )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.data_test,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def teardown(self, stage: Optional[str] = None):\n\"\"\"Clean up after fit or test.\"\"\"\n        pass\n\n    def state_dict(self):\n\"\"\"Extra things to save to checkpoint.\"\"\"\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n\"\"\"Things to do when loading checkpoint.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/openbb_chat/data/mnist_datamodule/#openbb_chat.data.mnist_datamodule.MNISTDataModule.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Things to do when loading checkpoint.</p> Source code in <code>openbb_chat/data/mnist_datamodule.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]):\n\"\"\"Things to do when loading checkpoint.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/openbb_chat/data/mnist_datamodule/#openbb_chat.data.mnist_datamodule.MNISTDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Download data if needed.</p> <p>Do not use it to assign state (self.x = y).</p> Source code in <code>openbb_chat/data/mnist_datamodule.py</code> <pre><code>def prepare_data(self):\n\"\"\"Download data if needed.\n\n    Do not use it to assign state (self.x = y).\n    \"\"\"\n    MNIST(self.hparams.data_dir, train=True, download=True)\n    MNIST(self.hparams.data_dir, train=False, download=True)\n</code></pre>"},{"location":"reference/openbb_chat/data/mnist_datamodule/#openbb_chat.data.mnist_datamodule.MNISTDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Load data. Set variables: <code>self.data_train</code>, <code>self.data_val</code>, <code>self.data_test</code>.</p> <p>This method is called by lightning with both <code>trainer.fit()</code> and <code>trainer.test()</code>, so be careful not to execute things like random split twice!</p> Source code in <code>openbb_chat/data/mnist_datamodule.py</code> <pre><code>def setup(self, stage: Optional[str] = None):\n\"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\n    This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n    careful not to execute things like random split twice!\n    \"\"\"\n    # load and split datasets only if not loaded already\n    if not self.data_train and not self.data_val and not self.data_test:\n        trainset = MNIST(self.hparams.data_dir, train=True, transform=self.transforms)\n        testset = MNIST(self.hparams.data_dir, train=False, transform=self.transforms)\n        dataset = ConcatDataset(datasets=[trainset, testset])\n        self.data_train, self.data_val, self.data_test = random_split(\n            dataset=dataset,\n            lengths=self.hparams.train_val_test_split,\n            generator=torch.Generator().manual_seed(42),\n        )\n</code></pre>"},{"location":"reference/openbb_chat/data/mnist_datamodule/#openbb_chat.data.mnist_datamodule.MNISTDataModule.state_dict","title":"<code>state_dict()</code>","text":"<p>Extra things to save to checkpoint.</p> Source code in <code>openbb_chat/data/mnist_datamodule.py</code> <pre><code>def state_dict(self):\n\"\"\"Extra things to save to checkpoint.\"\"\"\n    return {}\n</code></pre>"},{"location":"reference/openbb_chat/data/mnist_datamodule/#openbb_chat.data.mnist_datamodule.MNISTDataModule.teardown","title":"<code>teardown(stage=None)</code>","text":"<p>Clean up after fit or test.</p> Source code in <code>openbb_chat/data/mnist_datamodule.py</code> <pre><code>def teardown(self, stage: Optional[str] = None):\n\"\"\"Clean up after fit or test.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/openbb_chat/data/components/","title":"components","text":""},{"location":"reference/openbb_chat/llms/","title":"Index","text":""},{"location":"reference/openbb_chat/llms/guidance_wrapper/","title":"guidance_wrapper","text":""},{"location":"reference/openbb_chat/llms/guidance_wrapper/#openbb_chat.llms.guidance_wrapper.GuidanceWrapper","title":"<code>GuidanceWrapper</code>","text":"<p>Wrapper around <code>guidance</code> to be used with HF models.</p> Source code in <code>openbb_chat/llms/guidance_wrapper.py</code> <pre><code>class GuidanceWrapper:\n\"\"\"Wrapper around `guidance` to be used with HF models.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str = \"openlm-research/open_llama_3b_v2\",\n        tokenizer_kwargs: dict = {},\n        model_kwargs: dict = {},\n    ):\n\"\"\"Init method.\n\n        Args:\n            model_id (`str`):\n                Name of the HF model to use.\n        \"\"\"\n\n        tokenizer = AutoTokenizer.from_pretrained(model_id, **tokenizer_kwargs)\n\n        # set the default language model used to execute guidance programs\n        guidance.llm = guidance.llms.Transformers(\n            model=model_id, tokenizer=tokenizer, **model_kwargs\n        )\n\n    def __call__(self, *args, **kwargs):\n\"\"\"Calls `guidance.__call__` passing all the arguments to it.\"\"\"\n        return guidance(*args, **kwargs)\n</code></pre>"},{"location":"reference/openbb_chat/llms/guidance_wrapper/#openbb_chat.llms.guidance_wrapper.GuidanceWrapper.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Calls <code>guidance.__call__</code> passing all the arguments to it.</p> Source code in <code>openbb_chat/llms/guidance_wrapper.py</code> <pre><code>def __call__(self, *args, **kwargs):\n\"\"\"Calls `guidance.__call__` passing all the arguments to it.\"\"\"\n    return guidance(*args, **kwargs)\n</code></pre>"},{"location":"reference/openbb_chat/llms/guidance_wrapper/#openbb_chat.llms.guidance_wrapper.GuidanceWrapper.__init__","title":"<code>__init__(model_id='openlm-research/open_llama_3b_v2', tokenizer_kwargs={}, model_kwargs={})</code>","text":"<p>Init method.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>`str`</code> <p>Name of the HF model to use.</p> <code>'openlm-research/open_llama_3b_v2'</code> Source code in <code>openbb_chat/llms/guidance_wrapper.py</code> <pre><code>def __init__(\n    self,\n    model_id: str = \"openlm-research/open_llama_3b_v2\",\n    tokenizer_kwargs: dict = {},\n    model_kwargs: dict = {},\n):\n\"\"\"Init method.\n\n    Args:\n        model_id (`str`):\n            Name of the HF model to use.\n    \"\"\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_id, **tokenizer_kwargs)\n\n    # set the default language model used to execute guidance programs\n    guidance.llm = guidance.llms.Transformers(\n        model=model_id, tokenizer=tokenizer, **model_kwargs\n    )\n</code></pre>"},{"location":"reference/openbb_chat/models/","title":"Index","text":""},{"location":"reference/openbb_chat/models/mnist_module/","title":"mnist_module","text":""},{"location":"reference/openbb_chat/models/mnist_module/#openbb_chat.models.mnist_module.MNISTLitModule","title":"<code>MNISTLitModule</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>Example of LightningModule for MNIST classification.</p> A LightningModule organizes your PyTorch code into 6 sections <ul> <li>Initialization (init)</li> <li>Train Loop (training_step)</li> <li>Validation loop (validation_step)</li> <li>Test loop (test_step)</li> <li>Prediction Loop (predict_step)</li> <li>Optimizers and LR Schedulers (configure_optimizers)</li> </ul> Docs <p>https://lightning.ai/docs/pytorch/latest/common/lightning_module.html</p> Source code in <code>openbb_chat/models/mnist_module.py</code> <pre><code>class MNISTLitModule(LightningModule):\n\"\"\"Example of LightningModule for MNIST classification.\n\n    A LightningModule organizes your PyTorch code into 6 sections:\n        - Initialization (__init__)\n        - Train Loop (training_step)\n        - Validation loop (validation_step)\n        - Test loop (test_step)\n        - Prediction Loop (predict_step)\n        - Optimizers and LR Schedulers (configure_optimizers)\n\n    Docs:\n        https://lightning.ai/docs/pytorch/latest/common/lightning_module.html\n    \"\"\"\n\n    def __init__(\n        self,\n        net: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        scheduler: torch.optim.lr_scheduler,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        self.net = net\n\n        # loss function\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n        # metric objects for calculating and averaging accuracy across batches\n        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n        self.test_acc = Accuracy(task=\"multiclass\", num_classes=10)\n\n        # for averaging loss across batches\n        self.train_loss = MeanMetric()\n        self.val_loss = MeanMetric()\n        self.test_loss = MeanMetric()\n\n        # for tracking best so far validation accuracy\n        self.val_acc_best = MaxMetric()\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x)\n\n    def on_train_start(self):\n        # by default lightning executes validation step sanity checks before training starts,\n        # so it's worth to make sure validation metrics don't store results from these checks\n        self.val_loss.reset()\n        self.val_acc.reset()\n        self.val_acc_best.reset()\n\n    def model_step(self, batch: Any):\n        x, y = batch\n        logits = self.forward(x)\n        loss = self.criterion(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        return loss, preds, y\n\n    def training_step(self, batch: Any, batch_idx: int):\n        loss, preds, targets = self.model_step(batch)\n\n        # update and log metrics\n        self.train_loss(loss)\n        self.train_acc(preds, targets)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train/acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n\n        # return loss or backpropagation will fail\n        return loss\n\n    def on_train_epoch_end(self):\n        pass\n\n    def validation_step(self, batch: Any, batch_idx: int):\n        loss, preds, targets = self.model_step(batch)\n\n        # update and log metrics\n        self.val_loss(loss)\n        self.val_acc(preds, targets)\n        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val/acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n\n    def on_validation_epoch_end(self):\n        acc = self.val_acc.compute()  # get current val acc\n        self.val_acc_best(acc)  # update best so far val acc\n        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object\n        # otherwise metric would be reset by lightning after each epoch\n        self.log(\"val/acc_best\", self.val_acc_best.compute(), sync_dist=True, prog_bar=True)\n\n    def test_step(self, batch: Any, batch_idx: int):\n        loss, preds, targets = self.model_step(batch)\n\n        # update and log metrics\n        self.test_loss(loss)\n        self.test_acc(preds, targets)\n        self.log(\"test/loss\", self.test_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"test/acc\", self.test_acc, on_step=False, on_epoch=True, prog_bar=True)\n\n    def on_test_epoch_end(self):\n        pass\n\n    def configure_optimizers(self):\n\"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization.\n        Normally you'd need one. But in the case of GANs or similar you might have multiple.\n\n        Examples:\n            https://lightning.ai/docs/pytorch/latest/common/lightning_module.html#configure-optimizers\n        \"\"\"\n        optimizer = self.hparams.optimizer(params=self.parameters())\n        if self.hparams.scheduler is not None:\n            scheduler = self.hparams.scheduler(optimizer=optimizer)\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": {\n                    \"scheduler\": scheduler,\n                    \"monitor\": \"val/loss\",\n                    \"interval\": \"epoch\",\n                    \"frequency\": 1,\n                },\n            }\n        return {\"optimizer\": optimizer}\n</code></pre>"},{"location":"reference/openbb_chat/models/mnist_module/#openbb_chat.models.mnist_module.MNISTLitModule.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple.</p> <p>Examples:</p> <p>https://lightning.ai/docs/pytorch/latest/common/lightning_module.html#configure-optimizers</p> Source code in <code>openbb_chat/models/mnist_module.py</code> <pre><code>def configure_optimizers(self):\n\"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization.\n    Normally you'd need one. But in the case of GANs or similar you might have multiple.\n\n    Examples:\n        https://lightning.ai/docs/pytorch/latest/common/lightning_module.html#configure-optimizers\n    \"\"\"\n    optimizer = self.hparams.optimizer(params=self.parameters())\n    if self.hparams.scheduler is not None:\n        scheduler = self.hparams.scheduler(optimizer=optimizer)\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val/loss\",\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            },\n        }\n    return {\"optimizer\": optimizer}\n</code></pre>"},{"location":"reference/openbb_chat/models/components/","title":"Index","text":""},{"location":"reference/openbb_chat/models/components/simple_dense_net/","title":"simple_dense_net","text":""},{"location":"reference/openbb_chat/utils/","title":"Index","text":""},{"location":"reference/openbb_chat/utils/instantiators/","title":"instantiators","text":""},{"location":"reference/openbb_chat/utils/instantiators/#openbb_chat.utils.instantiators.instantiate_callbacks","title":"<code>instantiate_callbacks(callbacks_cfg)</code>","text":"<p>Instantiates callbacks from config.</p> Source code in <code>openbb_chat/utils/instantiators.py</code> <pre><code>def instantiate_callbacks(callbacks_cfg: DictConfig) -&gt; List[Callback]:\n\"\"\"Instantiates callbacks from config.\"\"\"\n\n    callbacks: List[Callback] = []\n\n    if not callbacks_cfg:\n        log.warning(\"No callback configs found! Skipping..\")\n        return callbacks\n\n    if not isinstance(callbacks_cfg, DictConfig):\n        raise TypeError(\"Callbacks config must be a DictConfig!\")\n\n    for _, cb_conf in callbacks_cfg.items():\n        if isinstance(cb_conf, DictConfig) and \"_target_\" in cb_conf:\n            log.info(f\"Instantiating callback &lt;{cb_conf._target_}&gt;\")\n            callbacks.append(hydra.utils.instantiate(cb_conf))\n\n    return callbacks\n</code></pre>"},{"location":"reference/openbb_chat/utils/instantiators/#openbb_chat.utils.instantiators.instantiate_loggers","title":"<code>instantiate_loggers(logger_cfg)</code>","text":"<p>Instantiates loggers from config.</p> Source code in <code>openbb_chat/utils/instantiators.py</code> <pre><code>def instantiate_loggers(logger_cfg: DictConfig) -&gt; List[Logger]:\n\"\"\"Instantiates loggers from config.\"\"\"\n\n    logger: List[Logger] = []\n\n    if not logger_cfg:\n        log.warning(\"No logger configs found! Skipping...\")\n        return logger\n\n    if not isinstance(logger_cfg, DictConfig):\n        raise TypeError(\"Logger config must be a DictConfig!\")\n\n    for _, lg_conf in logger_cfg.items():\n        if isinstance(lg_conf, DictConfig) and \"_target_\" in lg_conf:\n            log.info(f\"Instantiating logger &lt;{lg_conf._target_}&gt;\")\n            logger.append(hydra.utils.instantiate(lg_conf))\n\n    return logger\n</code></pre>"},{"location":"reference/openbb_chat/utils/logging_utils/","title":"logging_utils","text":""},{"location":"reference/openbb_chat/utils/logging_utils/#openbb_chat.utils.logging_utils.log_hyperparameters","title":"<code>log_hyperparameters(object_dict)</code>","text":"<p>Controls which config parts are saved by lightning loggers.</p> <p>Additionally saves: - Number of model parameters</p> Source code in <code>openbb_chat/utils/logging_utils.py</code> <pre><code>@rank_zero_only\ndef log_hyperparameters(object_dict: dict) -&gt; None:\n\"\"\"Controls which config parts are saved by lightning loggers.\n\n    Additionally saves:\n    - Number of model parameters\n    \"\"\"\n\n    hparams = {}\n\n    cfg = OmegaConf.to_container(object_dict[\"cfg\"])\n    model = object_dict[\"model\"]\n    trainer = object_dict[\"trainer\"]\n\n    if not trainer.logger:\n        log.warning(\"Logger not found! Skipping hyperparameter logging...\")\n        return\n\n    hparams[\"model\"] = cfg[\"model\"]\n\n    # save number of model parameters\n    hparams[\"model/params/total\"] = sum(p.numel() for p in model.parameters())\n    hparams[\"model/params/trainable\"] = sum(\n        p.numel() for p in model.parameters() if p.requires_grad\n    )\n    hparams[\"model/params/non_trainable\"] = sum(\n        p.numel() for p in model.parameters() if not p.requires_grad\n    )\n\n    hparams[\"data\"] = cfg[\"data\"]\n    hparams[\"trainer\"] = cfg[\"trainer\"]\n\n    hparams[\"callbacks\"] = cfg.get(\"callbacks\")\n    hparams[\"extras\"] = cfg.get(\"extras\")\n\n    hparams[\"task_name\"] = cfg.get(\"task_name\")\n    hparams[\"tags\"] = cfg.get(\"tags\")\n    hparams[\"ckpt_path\"] = cfg.get(\"ckpt_path\")\n    hparams[\"seed\"] = cfg.get(\"seed\")\n\n    # send hparams to all loggers\n    for logger in trainer.loggers:\n        logger.log_hyperparams(hparams)\n</code></pre>"},{"location":"reference/openbb_chat/utils/pylogger/","title":"pylogger","text":""},{"location":"reference/openbb_chat/utils/pylogger/#openbb_chat.utils.pylogger.get_pylogger","title":"<code>get_pylogger(name=__name__)</code>","text":"<p>Initializes multi-GPU-friendly python command line logger.</p> Source code in <code>openbb_chat/utils/pylogger.py</code> <pre><code>def get_pylogger(name=__name__) -&gt; logging.Logger:\n\"\"\"Initializes multi-GPU-friendly python command line logger.\"\"\"\n\n    logger = logging.getLogger(name)\n\n    # this ensures all logging levels get marked with the rank zero decorator\n    # otherwise logs would get multiplied for each GPU process in multi-GPU setup\n    logging_levels = (\"debug\", \"info\", \"warning\", \"error\", \"exception\", \"fatal\", \"critical\")\n    for level in logging_levels:\n        setattr(logger, level, rank_zero_only(getattr(logger, level)))\n\n    return logger\n</code></pre>"},{"location":"reference/openbb_chat/utils/rich_utils/","title":"rich_utils","text":""},{"location":"reference/openbb_chat/utils/rich_utils/#openbb_chat.utils.rich_utils.enforce_tags","title":"<code>enforce_tags(cfg, save_to_file=False)</code>","text":"<p>Prompts user to input tags from command line if no tags are provided in config.</p> Source code in <code>openbb_chat/utils/rich_utils.py</code> <pre><code>@rank_zero_only\ndef enforce_tags(cfg: DictConfig, save_to_file: bool = False) -&gt; None:\n\"\"\"Prompts user to input tags from command line if no tags are provided in config.\"\"\"\n\n    if not cfg.get(\"tags\"):\n        if \"id\" in HydraConfig().cfg.hydra.job:\n            raise ValueError(\"Specify tags before launching a multirun!\")\n\n        log.warning(\"No tags provided in config. Prompting user to input tags...\")\n        tags = Prompt.ask(\"Enter a list of comma separated tags\", default=\"dev\")\n        tags = [t.strip() for t in tags.split(\",\") if t != \"\"]\n\n        with open_dict(cfg):\n            cfg.tags = tags\n\n        log.info(f\"Tags: {cfg.tags}\")\n\n    if save_to_file:\n        with open(Path(cfg.paths.output_dir, \"tags.log\"), \"w\") as file:\n            rich.print(cfg.tags, file=file)\n</code></pre>"},{"location":"reference/openbb_chat/utils/rich_utils/#openbb_chat.utils.rich_utils.print_config_tree","title":"<code>print_config_tree(cfg, print_order=('data', 'model', 'callbacks', 'logger', 'trainer', 'paths', 'extras'), resolve=False, save_to_file=False)</code>","text":"<p>Prints content of DictConfig using Rich library and its tree structure.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration composed by Hydra.</p> required <code>print_order</code> <code>Sequence[str]</code> <p>Determines in what order config components are printed.</p> <code>('data', 'model', 'callbacks', 'logger', 'trainer', 'paths', 'extras')</code> <code>resolve</code> <code>bool</code> <p>Whether to resolve reference fields of DictConfig.</p> <code>False</code> <code>save_to_file</code> <code>bool</code> <p>Whether to export config to the hydra output folder.</p> <code>False</code> Source code in <code>openbb_chat/utils/rich_utils.py</code> <pre><code>@rank_zero_only\ndef print_config_tree(\n    cfg: DictConfig,\n    print_order: Sequence[str] = (\n        \"data\",\n        \"model\",\n        \"callbacks\",\n        \"logger\",\n        \"trainer\",\n        \"paths\",\n        \"extras\",\n    ),\n    resolve: bool = False,\n    save_to_file: bool = False,\n) -&gt; None:\n\"\"\"Prints content of DictConfig using Rich library and its tree structure.\n\n    Args:\n        cfg (DictConfig): Configuration composed by Hydra.\n        print_order (Sequence[str], optional): Determines in what order config components are printed.\n        resolve (bool, optional): Whether to resolve reference fields of DictConfig.\n        save_to_file (bool, optional): Whether to export config to the hydra output folder.\n    \"\"\"\n\n    style = \"dim\"\n    tree = rich.tree.Tree(\"CONFIG\", style=style, guide_style=style)\n\n    queue = []\n\n    # add fields from `print_order` to queue\n    for field in print_order:\n        queue.append(field) if field in cfg else log.warning(\n            f\"Field '{field}' not found in config. Skipping '{field}' config printing...\"\n        )\n\n    # add all the other fields to queue (not specified in `print_order`)\n    for field in cfg:\n        if field not in queue:\n            queue.append(field)\n\n    # generate config tree from queue\n    for field in queue:\n        branch = tree.add(field, style=style, guide_style=style)\n\n        config_group = cfg[field]\n        if isinstance(config_group, DictConfig):\n            branch_content = OmegaConf.to_yaml(config_group, resolve=resolve)\n        else:\n            branch_content = str(config_group)\n\n        branch.add(rich.syntax.Syntax(branch_content, \"yaml\"))\n\n    # print config tree\n    rich.print(tree)\n\n    # save config tree to file\n    if save_to_file:\n        with open(Path(cfg.paths.output_dir, \"config_tree.log\"), \"w\") as file:\n            rich.print(tree, file=file)\n</code></pre>"},{"location":"reference/openbb_chat/utils/utils/","title":"utils","text":""},{"location":"reference/openbb_chat/utils/utils/#openbb_chat.utils.utils.extras","title":"<code>extras(cfg)</code>","text":"<p>Applies optional utilities before the task is started.</p> <p>Utilities: - Ignoring python warnings - Setting tags from command line - Rich config printing</p> Source code in <code>openbb_chat/utils/utils.py</code> <pre><code>def extras(cfg: DictConfig) -&gt; None:\n\"\"\"Applies optional utilities before the task is started.\n\n    Utilities:\n    - Ignoring python warnings\n    - Setting tags from command line\n    - Rich config printing\n    \"\"\"\n\n    # return if no `extras` config\n    if not cfg.get(\"extras\"):\n        log.warning(\"Extras config not found! &lt;cfg.extras=null&gt;\")\n        return\n\n    # disable python warnings\n    if cfg.extras.get(\"ignore_warnings\"):\n        log.info(\"Disabling python warnings! &lt;cfg.extras.ignore_warnings=True&gt;\")\n        warnings.filterwarnings(\"ignore\")\n\n    # prompt user to input tags from command line if none are provided in the config\n    if cfg.extras.get(\"enforce_tags\"):\n        log.info(\"Enforcing tags! &lt;cfg.extras.enforce_tags=True&gt;\")\n        rich_utils.enforce_tags(cfg, save_to_file=True)\n\n    # pretty print config tree using Rich library\n    if cfg.extras.get(\"print_config\"):\n        log.info(\"Printing config tree with Rich! &lt;cfg.extras.print_config=True&gt;\")\n        rich_utils.print_config_tree(cfg, resolve=True, save_to_file=True)\n</code></pre>"},{"location":"reference/openbb_chat/utils/utils/#openbb_chat.utils.utils.get_metric_value","title":"<code>get_metric_value(metric_dict, metric_name)</code>","text":"<p>Safely retrieves value of the metric logged in LightningModule.</p> Source code in <code>openbb_chat/utils/utils.py</code> <pre><code>def get_metric_value(metric_dict: dict, metric_name: str) -&gt; float:\n\"\"\"Safely retrieves value of the metric logged in LightningModule.\"\"\"\n\n    if not metric_name:\n        log.info(\"Metric name is None! Skipping metric value retrieval...\")\n        return None\n\n    if metric_name not in metric_dict:\n        raise Exception(\n            f\"Metric value not found! &lt;metric_name={metric_name}&gt;\\n\"\n            \"Make sure metric name logged in LightningModule is correct!\\n\"\n            \"Make sure `optimized_metric` name in `hparams_search` config is correct!\"\n        )\n\n    metric_value = metric_dict[metric_name].item()\n    log.info(f\"Retrieved metric value! &lt;{metric_name}={metric_value}&gt;\")\n\n    return metric_value\n</code></pre>"},{"location":"reference/openbb_chat/utils/utils/#openbb_chat.utils.utils.task_wrapper","title":"<code>task_wrapper(task_func)</code>","text":"<p>Optional decorator that controls the failure behavior when executing the task function.</p> <p>This wrapper can be used to: - make sure loggers are closed even if the task function raises an exception (prevents multirun failure) - save the exception to a <code>.log</code> file - mark the run as failed with a dedicated file in the <code>logs/</code> folder (so we can find and rerun it later) - etc. (adjust depending on your needs)</p> <p>Example:</p> <pre><code>@utils.task_wrapper\ndef train(cfg: DictConfig) -&gt; Tuple[dict, dict]:\n\n    ...\n\n    return metric_dict, object_dict\n</code></pre> Source code in <code>openbb_chat/utils/utils.py</code> <pre><code>def task_wrapper(task_func: Callable) -&gt; Callable:\n\"\"\"Optional decorator that controls the failure behavior when executing the task function.\n\n    This wrapper can be used to:\n    - make sure loggers are closed even if the task function raises an exception (prevents multirun failure)\n    - save the exception to a `.log` file\n    - mark the run as failed with a dedicated file in the `logs/` folder (so we can find and rerun it later)\n    - etc. (adjust depending on your needs)\n\n    Example:\n    ```\n    @utils.task_wrapper\n    def train(cfg: DictConfig) -&gt; Tuple[dict, dict]:\n\n        ...\n\n        return metric_dict, object_dict\n    ```\n    \"\"\"\n\n    def wrap(cfg: DictConfig):\n        # execute the task\n        try:\n            metric_dict, object_dict = task_func(cfg=cfg)\n\n        # things to do if exception occurs\n        except Exception as ex:\n            # save exception to `.log` file\n            log.exception(\"\")\n\n            # some hyperparameter combinations might be invalid or cause out-of-memory errors\n            # so when using hparam search plugins like Optuna, you might want to disable\n            # raising the below exception to avoid multirun failure\n            raise ex\n\n        # things to always do after either success or exception\n        finally:\n            # display output dir path in terminal\n            log.info(f\"Output dir: {cfg.paths.output_dir}\")\n\n            # always close wandb run (even if exception occurs so multirun won't fail)\n            if find_spec(\"wandb\"):  # check if wandb is installed\n                import wandb\n\n                if wandb.run:\n                    log.info(\"Closing wandb!\")\n                    wandb.finish()\n\n        return metric_dict, object_dict\n\n    return wrap\n</code></pre>"}]}